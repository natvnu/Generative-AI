
'''
Manually corrected issues:
1. AI  could not automatically detect that corr() function is only able to determine correlation 
between numerical variables, so the code generated by AI caused an error. 
Prompt had to be changed to exclude categorical variables

2. AI used inplace=True which raised a warning. Code had to be altered.

3. AI used 'Fuel Type' as a name of the column, instead of 'fuelType'. Code had to be altered.

4. AI used 'Price' instead of the 'price', 'Transmissiom Method' instead of 'transmission', etc...

5. AI does not keep clear track of previous prompts so the code it generates often needs to be altered,
or prompts need to be very detailed

6. parts of the code can be reused, which saves computational time, 
AI is not exactly capable to do this independently, because it does not have access to code editor

'''

# Import necessary libraries
%pip install seaborn

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import requests
from io import StringIO
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures

from sklearn.linear_model import Ridge

from sklearn.model_selection import GridSearchCV, train_test_split


# Function to load dataset from URL
def load_dataset(url):
    try:
        # Send HTTP request to fetch data from the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an error for bad status codes
        
        # Use StringsIO to handle the content of response as file-like object
        data_io = StringIO(response.text)
        
        # Load data into pandas DataFrame
        df = pd.read_csv(data_io, header=0)
        
        return df
        
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data: {e}")
        return None
    except pd.errors.ParserError:
        print("Error parsing the data, check the URL or file format.")
        return None
# Example usage
url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0271EN-SkillsNetwork/labs/v1/m3/data/used_car_price_analysis.csv"
df = load_dataset(url)

#data preprocessing
def clean_dataset(df):
    # Identify columns with missing values
    missing_vals = df.isnull().sum()
    #print("Columns with missing values:\n", missing_vals[missing_vals > 0])

    # Fill missing values with the mean of respective columns
    for col in missing_vals[missing_vals > 0].index:
        mean_val = df[col].mean()
        df[col]=df[col].fillna(mean_val)

    # Drop duplicate rows
    df_no_duplicates = df.drop_duplicates()

    # Optionally, confirm the removal of duplicates
    #if df.shape[0] != df_no_duplicates.shape[0]:
    #    print(f"Dropping {df.shape[0] - df_no_duplicates.shape[0]} duplicate rows.")
    #else:
    #    print("No duplicate rows found.")
  
    return df_no_duplicates
    
cleaned_df = clean_dataset(df)
cleaned_df.shape

#this dataset can be augmented to include more synthetically generated observiations using a tool like MOSTLY.AI
#cleaned_df.to_csv('out.csv', index=False) #export the csv to upload in MOSTLY.AI
'''
#identify the vars with highest correlation to target var
def identify_high_corr_numeric_attributes(df):
    # Separate numeric and categorical columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    
    # Compute the correlation matrix for only numeric columns
    corr_matrix = df[numeric_cols].corr()
    
    # Selecting 'price' as the target column
    price_corr = corr_matrix['price'].sort_values(ascending=False)
    
    # Filter correlations to follow only the strong ones (abs > 0.2)
    filtered_corr = price_corr[price_corr > 0.2]
    
    # Get the top 5 most correlated attributes
    top_correlated_features = filtered_corr.index[:5]
    
    return top_correlated_features

correlation_results = identify_high_corr_numeric_attributes(cleaned_df)
#print("Top 5 Numeric Attributes most correlated with price:")
#print(correlation_results)

    
# Print the counts
#print("Number of observations by Fuel Type:\n", cleaned_df['fuelType'].value_counts())

#box plot by transmission
plt.figure(figsize=(12, 8))
sns.boxplot(x='transmission', y='price', data=cleaned_df)
plt.title('Price Distribution by Transmission Type')
plt.xlabel('Transmission Type')
plt.ylabel('Price')
plt.show()

#  Regression plot mpg vs price:
plt.figure(figsize=(10, 6))
sns.regplot(x='mpg', y='price', data=cleaned_df, ci=None)
plt.title('Regression Plot: MPG vs Price')
plt.xlabel('Miles per Gallon (mpg)')
plt.ylabel('Price')
plt.show()
''' 

#simple linear regression- for simple and multiple linear regression, the difference is only in x and y definition
# Define features (X) and target (y)
#X = cleaned_df[['mpg']]
#y = cleaned_df['price']


#multiple linear regression 
features = ['year', 'mileage', 'tax', 'mpg', 'engineSize']
X = df[features]
y = df['price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


'''
# Initialize and fit the Linear Regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predict on the test set
y_pred = linear_model.predict(X_test)

# Calculate R^2 and MSE
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Print the results
#print('Multiple linear regression')
#print(f"R^2 Score: {r2:.4f}") #0.7257
#print(f"Mean Squared Error (MSE): {mse:.4f}") #6148224.5602


#create and fit the pipeline
# X and y we have already defined
# Define the pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()), 
    ('poly', PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)), 
    ('linreg', LinearRegression()) 
])

# Split the data into training and testing sets - not required as we have it above
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the pipeline
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Print the results
#print('Pipeline containing polynomial of 2nd degree')
print(f"R^2 Score: {r2:.4f}") #0.7527
print(f"Mean Squared Error (MSE): {mse:.4f}") #5543050.8668


#create and fit the Ridge Regression Model
# Initialize Ridge Regression model with alpha=0.1
ridge_model = Ridge(alpha=0.1)

# Fit the model to the training data
ridge_model.fit(X_train, y_train)

# Predict the target variable for the test set
y_pred = ridge_model.predict(X_test)

# Calculate R^2 score for the predictions
r2 = r2_score(y_test, y_pred)

# Print the R^2 value
print(f"R^2 Score: {r2:.4f}")#0.7257


# polynomially transform training and testing data and then create and fit Ridge regression object (alpha=0.1)
# then calculate the R^2 and MSE utilising the transformed test data
# Apply PolynomialFeatures to both training and testing data for second-order terms
poly = PolynomialFeatures(degree=2, include_bias=False)

X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Initialize Ridge Regression model with alpha=0.1
ridge_model = Ridge(alpha=0.1)

# Fit the model to the polynomial-transformed training data
ridge_model.fit(X_train_poly, y_train)

# Predict the target variable for the polynomial-transformed test set
y_pred = ridge_model.predict(X_test_poly)

# Calculate R^2 score for the predictions
r2 = r2_score(y_test, y_pred)

# Calculate MSE for the predictions
mse = mean_squared_error(y_test, y_pred)

# Print the evaluation metrics
print(f"R^2 Score: {r2:.4f}") #0.7525
print(f"Mean Squared Error (MSE): {mse:.4f}") #5546814.7140

'''

# for the question above use grid search with cv=4 to find the best alpha among (0.01, 0.1, 1, 10, 100)
# Define the parameter grid for GridSearchCV
param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}

# Apply PolynomialFeatures to both training and testing data for second-order terms
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train) #we do this again
X_test_poly = poly.transform(X_test) #we do this again

# Initialize Ridge Regression and Grid Search CV
ridge = Ridge()
grid_search = GridSearchCV(ridge, param_grid, cv=4, scoring='r2', return_train_score=True)

# Fit GridSearchCV on the polynomial-transformed training data
grid_search.fit(X_train_poly, y_train)

# Get the best model from GridSearchCV
best_ridge = grid_search.best_estimator_

# Predict using the best model on the polynomial-transformed test data
y_pred = best_ridge.predict(X_test_poly)

# Calculate R^2 and MSE for the predictions
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Print the evaluation metrics
print(f"Best Alpha: {grid_search.best_params_['alpha']:.4f}") #100.0000
print(f"R^2 Score: {r2:.4f}") #0.7554
print(f"Mean Squared Error (MSE): {mse:.4f}")# 5481845.0981

'''
Conclusion: The best performing model is Ridge Regression with alpha 100.
Changing the polynomial transformation degree to 3 instead of 2 would provide evenbetter results:
Best Alpha: 1.0000
R^2 Score: 0.7949
Mean Squared Error (MSE): 4596039.2222
''
